# ACM ICN 2020 DEMO

- slack: [icn2020workspace.slack.com](icn2020workspace.slack.com)
  - #session_3-5_chipmunk
  - https://app.slack.com/client/T01817WRARE/C01B2DHEYPP
  
- Youtube: [ACM ICN 2020 - (Demo) Chipmunk: Distributed Object Storage for NDN](https://youtu.be/VQuHfbSbkj8)

## 의견 (slack)

> 가능한 슬랙 채널을 사용해서 보기를 권장합니다.

- Q: by Junxiao Shi (yoursunny)
  - _While the data(segment) is being stored, its name is prefixed by node’s ID such as ‘/node-name.’_
  - This would break the signature.
  - Who advertises the name /hello into routing system? Or does this rely on self-learning / broadcast?
  - A:
    - Actually,  stored Data  packet is encapsulated with new name with node-name. Encapsulated data use sha256 digest sig_type. So the original Data is untacted.
    - The service name is advertized to routing system by each chipmunk server.

- Q: by Junxiao Shi (yoursunny)
  - Each Chipmunk server could advertise /chipmunk, but do they also advertise /hello?

- Q: by lixia
  - if users are to pick among multiple replicates, then how the load balancing is done?

- Q: by Susmit Shannigrahi
  - Why ask for metadata and not data directly?

- Q: by Junxiao Shi (yoursunny)
  - If the producer needs to be online to advertise /hello, then the repo alone cannot serve the Data /hello as soon as the producer goes offline.

- Q: by lixia
  - @YONGYOON SHIN **we are revising repo-ng spec as we speak, let's discuss how to collaborate.**
  - A:
    - @lixia Welcome. I really want to collaborate.
  
- Q: by Junxiao Shi (yoursunny)
  - It's not repo-ng anymore. https://ndn-python-repo.readthedocs.io/
  - A:
    - Thank you for the information on ndn-python-repo.

- A:
  - The producer specifies the service name (/hello) when inserting the data. Upon receiving this request, the Chipmunks server advertises the service name (/hello) to the router.
  - When advertising with the router, the service name includes the name of the node where the data was stored.
(Names of the same data distributed across the network do not duplicates because they contain the names of the nodes where the data was stored.)
  - When a consumer requests data (/hello) using a common prefix (/chipmonunk), it hashes the service name /hello and responds with a metadata file.
  - Metadata contains information about the data for the service name. Consumers use metadata files to determine which nodes are storing data.
(can create a list of nodes that have the same data.)
  - The consumer sends an Interest via metadata to a data name, including the node name.
  - We are planning a mechanism for load balancing the same data distributed in the network.
  
- Q: by Junxiao Shi (yoursunny)
  - Are you saying, the Interest name sent from the consumer program is not /hello , but /chipmunk/H(/hello) ?
  - What is a metadata file? Is it the same concept as a FLIC manifest? Or what's the difference?
  - A:
    - Yes , all Interest from consumer and producer to chiipmunck server has name starting /chipmucnk/..., for sroring and retrieving.

- Q: by Junxiao Shi (yoursunny)
  - I see. In this case, the servers do not need to advertise /hello into routing. They just need to advertise /chipmunk .
  - A:
    - Metadata is a manifest file which has information about the "data name" which server stores with what name (including real node name).

- Q: by Junxiao Shi (yoursunny)
  - Is /chipmunk in this case a configurable name (each installation can have something different), or a well-known name (fixed in protocol)?
  - I got confused because your ndnputfile  command line contains /chipmunk but your ndngetfile command line does not contain it.
  - This is also why I thought the consumer is just sending Interest /hello.
  - A:
    - Service name is configurable on installation, and assume the consumer and producer knows the name.  The ndngetfile use the configuration file, and /chipmunk is in the configuration file.
    - This is because authors try to follow the repo-ng commands as is.
    - We will change the getfile command to include /servicename .

- Q: by Junxiao Shi (yoursunny)
  - I suppose Chipmunk codebase is not based on repo-ng?
  - I see the storage is mostly POSIX filesystem. This could lead to bottleneck if you are storing many small objects.
  - My repo uses LevelDB. https://ndnts-docs.ndn.today/typedoc/modules/repo.html
  - ndn-python-repo can LevelDB or Mongo.
    - [ndnts-docs.ndn.today](https://ndnts-docs.ndn.today/typedoc/modules/repo.html)
  - A:
    - Actually it is based on repo-ng, except that it dosen't use sql DB.

- Q: by Junxiao Shi (yoursunny)
  - How does DHT nodes discover each other? Is it NDN based?
  - A:
    - We assume that chipmunk  is the managed service by service provider. So all server knows each other by configuration or management.
  - You can consider integrating PSync to easier deployment.
  - I can see that your current design has to be managed be the same provider, because it does not have automatic repartitioning.
  - Q:
    - I think PSync also requires all the node should know the other servers before running.
    - A: by Junxiao Shi
      - PSync has two separate protocols: FullSync and PartialSync.
      - The one you need is the FullSync protocol. Nodes only need to know the sync group name. They can then discover each other.
    - Other Sync has scalability issues, if the servers are spreaded in multiple networks, Full sync is based on flooding. that is the problem.
  - DHT based systems would never work well in "servers spread in multiple networks" scenario.
    - A:
      - We are thinking of managed DHT,  controlled by a single service provider. This can be applied to multiple network scenario.
      - Current we are implementing a managed DHT library.
  - On the retrieval side, nTorrent has some similarity to your problem.
  - They don't use DHT, but just advertise a thousand names into routing, or rely on network to figure out where the data pieces are.
    - https://named-data.net/publications/2017-icccn-ntorrent/
  - After all, NDN has caching, so that it is not really necessary to figure how which server hosts the data.
    - A:
      - I'm not sure but nTorrent may be based on Sync, and I think it is only for single network solution.
    - nTorrent is designed before PSync. They don't have sync.
    - A:
      - Ok,  I'll chek later,  but when we read the paper, I thought it dosen't scale well, but I can't remember why.
    - Yes, nTorrent cannot scale well, because it either needs one routing announcement per packet, or perform self-learning (or get Nacks) per packet.
  - Q: to Junxiao Shi
    - If we use NDNS to recorded the stored server, then consumer can get data directly from the server. But this require the producer shoud know the stored server, and update NDNS.
    - A: by Junxiao Shi
      - Your solution, on the other hand, either has single point of failure (if the designated node goes down), or breaks caching (if multiple nodes store the same data but each has a different prefix).
    - We think that the storage service user just want to store and retrieve the data, not knowing the details.
    - A: by Junxiao Shi
      - You don't need NDNS. Instead, use **forwarding hint**.
      - Data name is always /hello/v=2/seg=0.
      - In the manifest, the server tells consumers that "segment 0 is available on server 3 and server 5".
      - Interest would be name= /hello/v=2/seg=0 with forwarding hint /chipmunk3, /chipmunk5.
      - The network can forward this Interest to either of these servers.
      - By using forwarding hint, you can avoid encapsulation.
  - Q: to Junxiao Shi
    - Yes, if we use forwarding hint, we don't need to encapsulate with new name. But this ckid of forwarding hint method is using it for server redirection. What if the server is located in other network? We need forwading hint rewriting, then who do the rewriting? Some kind of hierarchical fowarding hint mechanism is required
    - A:
      - You don't need forwarding hint rewriting.
      - Each server can be reached through a location-dependent name.
      - For example, /ndn/edu/arizona/chipmunk3 , /ndn/fr/lip6/chipmunk5 .
    - Do you think each server's location-dependent name should be advertized to the core network ?
    - It's wrong question. Server-name dosen't needeed to be advertized in the core network, because LPM is also applied for forwarding hint FIB lookup.
    - Yes, actually we also consider the forwarding hint approach as an alternate.
    - I agree with your comments.  I thought that it requires rewriting,  Thank you so much
    - A:
      - Many people incorrectly believe that forwarding hint delegation name must equal network name such as /ndn/edu/arizona, but this isn't true. It could identify a network, a host, or even a program.

- Q: to Junxiao Shi
  - Do you think noSQL DB is fast than the filesystem?
  - A: by Junxiao Shi
    - Filesystem is inefficient if you have millions of 1KB files.
    - If that's not the case, git would not have packfiles.
      - https://git-scm.com/book/en/v2/Git-Internals-Packfiles
      - A: git uses packfiles to save space, not because they are faster...
  - A:
    - Our file system follows git-style. I'll check the link later with authors.
  - A: by Junxiao Shi
    - I hear the Python repo has scaled to terabytes, using MongoDB storage.


```
Alex Afanasyev  3:36 AM
https://youtu.be/VQuHfbSbkj8
YouTubeYouTube | Association for Computing Machinery (ACM)
ACM ICN 2020 - (Demo) Chipmunk: Distributed Object Storage for NDN 

yoursunny  3:39 AM
While the data(segment) is being stored, its name is prefixed by node’s ID such as‘/node-name.’
This would break the signature.
3:43
Who advertises the name /hello into routing system? Or does this rely on self-learning / broadcast?

Sung Hyuk Byun  3:44 AM
Actually,  stored Data  packet is encapsulated with new name with node-name.  Encapsulated data use sha256 digest sig_type.   So the original Data is untacted.
3:45
The service name is advertized to routing system by each chipmunk server

yoursunny  3:45 AM
Each ChipMunk server could advertise /chipmunk, but do they also advertise /hello? (edited) 

lixia  3:47 AM
if users are to pick among multiple replicates, then how the load balancing is done?

Susmit Shannigrahi  3:48 AM
Why ask for metadata and not data directly?

yoursunny  3:49 AM
If the producer needs to be online to advertise /hello, then the repo alone cannot serve the Data /hello as soon as the producer goes offline.

lixia  3:50 AM
@YONGYOON SHIN we are revising repo-ng spec as we speak, let's discuss how to collaborate.

yoursunny  3:51 AM
It's not repo-ng anymore. https://ndn-python-repo.readthedocs.io/

YONGYOON SHIN  3:53 AM
@lixia Welcome. I really want to collaborate.

YONGYOON SHIN  4:01 AM
Thank you for the information on ndn-python-repo.

Justin Park  4:26 AM
@yoursunny yes, Chipmunk server do so. Thanks.

YONGYOON SHIN  5:26 AM
The producer specifies the service name (/hello) when inserting the data. Upon receiving this request, the Chipmunks server advertises the service name (/hello) to the router.
When advertising with the router, the service name includes the name of the node where the data was stored.
(Names of the same data distributed across the network do not duplicates because they contain the names of the nodes where the data was stored.)
When a consumer requests data (/hello) using a common prefix (/chipmonunk), it hashes the service name /hello and responds with a metadata file.
Metadata contains information about the data for the service name. Consumers use metadata files to determine which nodes are storing data.
(can create a list of nodes that have the same data.)
The consumer sends an Interest via metadata to a data name, including the node name.
We are planning a mechanism for load balancing the same data distributed in the network. (edited) 

yoursunny  9:02 AM
Are you saying, the Interest name sent from the consumer program is not /hello , but /chipmunk/H(/hello) ?
9:02
What is a metadata file? Is it the same concept as a FLIC manifest? Or what's the difference?

Sung Hyuk Byun  9:04 AM
Yes , all Interest from consumer and producer to chiipmunck server has name starting /chipmucnk/...   , for sroring and retrieving .

yoursunny  9:05 AM
I see. In this case, the servers do not need to advertise /hello into routing. They just need to advertise /chipmunk .

Sung Hyuk Byun  9:06 AM
Metadata is a manifest file which has information about the "data name" which server stores with what name (including real node name)

yoursunny  9:08 AM
Is /chipmunk in this case a configurable name (each installation can have something different), or a well-known name (fixed in protocol)?
I got confused because your ndnputfile  command line contains /chipmunk but your ndngetfile command line does not contain it.
This is also why I thought the consumer is just sending Interest /hello.

Sung Hyuk Byun  9:10 AM
Service name is configurable on installation, and assume the consumer and producer knows the name.  The ndngetfile use the configuration file, and /chipmunk is in the configuration file.
9:11
This is becuase authors try to follow the repo-ng commands as is.
9:11
We will change the getfile command to include /servicename

yoursunny  9:12 AM
I suppose Chipmunk codebase is not based on repo-ng?
9:14
I see the storage is mostly POSIX filesystem. This could lead to bottleneck if you are storing many small objects.
My repo uses LevelDB. https://ndnts-docs.ndn.today/typedoc/modules/repo.html
ndn-python-repo can LevelDB or Mongo.
ndnts-docs.ndn.todayndnts-docs.ndn.today
repo | NDNts
Documentation for NDNts

Sung Hyuk Byun  9:14 AM
Actually it is based on repo-ng, except that it dosen't use sql DB

yoursunny  9:15 AM
How does DHT nodes discover each other? Is it NDN based?

Sung Hyuk Byun  9:15 AM
Do you think noSQL DB is fast than the filesystem?

yoursunny  9:16 AM
Filesystem is inefficient if you have millions of 1KB files.
If that's not the case, git would not have packfiles.
https://git-scm.com/book/en/v2/Git-Internals-Packfiles

Sung Hyuk Byun  9:18 AM
Our file system follows git-style .  I'll check the link later with authors

yoursunny  9:19 AM
I hear the Python repo has scaled to terabytes, using MongoDB storage.
9:19
How does DHT nodes discover each other? Is it NDN based?

Sung Hyuk Byun  9:20 AM
We assume that chipmunk  is the managed service by service provider. So all server knows each other by configuration or management

yoursunny  9:22 AM
You can consider integrating PSync to easier deployment.
I can see that your current design has to be managed be the same provider, because it does not have automatic repartitioning.

Sung Hyuk Byun  9:24 AM
I think PSync also requires all the node should know the other servers before running

yoursunny  9:25 AM
PSync has two separate protocols: FullSync and PartialSync.
The one you need is the FullSync protocol. Nodes only need to know the sync group name. They can then discover each other.

Sung Hyuk Byun  9:25 AM
Other Sync has scalability issues, if the servers are spreaded in multiple networks,   Full sync is based on flooding.  that is the problem.

yoursunny  9:26 AM
DHT based systems would never work well in "servers spread in multiple networks" scenario.

Sung Hyuk Byun  9:28 AM
We are thinking of managed DHT,  controlled by a single service provider. This can be applied to multiple network scenario
9:29
Current we are implementing a managed DHT library.

yoursunny  9:30 AM
On the retrieval side, nTorrent has some similarity to your problem.
They don't use DHT, but just advertise a thousand names into routing, or rely on network to figure out where the data pieces are.
https://named-data.net/publications/2017-icccn-ntorrent/
Named Data Networking (NDN)Named Data Networking (NDN)
nTorrent: Peer-to-Peer File Sharing in Named Data Networking - Named Data Networking (NDN)
nTorrent: Peer-to-Peer File Sharing in Named Data Networking
9:30
After all, NDN has caching, so that it is not really necessary to figure how which server hosts the data.
9:32
I think this is why @Susmit Shannigrahi suggests retrieving data directly.

Sung Hyuk Byun  9:33 AM
I'm not sure but nTorrent may be based on Sync, and I think it is only for single network solution .

yoursunny  9:33 AM
nTorrent is designed before PSync. They don't have sync.

Sung Hyuk Byun  9:35 AM
Ok,  I'll chek later,  but when we read the paper, I thought it dosen't scale well, but I can't remember why.

yoursunny  9:37 AM
Yes, nTorrent cannot scale well, because it either needs one routing announcement per packet, or perform self-learning (or get Nacks) per packet.

Sung Hyuk Byun  9:38 AM
If we use NDNS to recorded the stored server, then consumer can get data directly from the server.  But this require the producer shoud know the stored server, and update NDNS.

yoursunny  9:38 AM
Your solution, on the other hand, either has single point of failure (if the designated node goes down), or breaks caching (if multiple nodes store the same data but each has a different prefix).

Sung Hyuk Byun  9:40 AM
We think that the storage service user just want to store and retrieve the data, not knowing the details.

yoursunny  9:40 AM
You don't need NDNS. Instead, use forwarding hint.
Data name is always /hello/v=2/seg=0.
In the manifest, the server tells consumers that "segment 0 is available on server 3 and server 5".
Interest would be name= /hello/v=2/seg=0 with forwarding hint /chipmunk3 , /chipmunk5.
The network can forward this Interest to either of these servers.
9:42
By using forwarding hint, you can avoid encapsulation.

Sung Hyuk Byun  9:46 AM
Yes,  if we use forwarding hint, we don't need to encapsulate with new name. But this ckid of forwarding hint method is using it for server redirection.  What if the server is located in other network?   We need forwading hint rewriting, then who do the rewriting?   Some kind of hierarchical fowarding hint mechanism is required

yoursunny  9:47 AM
You don't need forwarding hint rewriting.
Each server can be reached through a location-dependent name.
For example, /ndn/edu/arizona/chipmunk3 , /ndn/fr/lip6/chipmunk5 .

Sung Hyuk Byun  9:50 AM
Do you think each server's location-dependent name should be advertized to the core network ?
9:53
It's wrong question.  Server-name dosen't needeed to be advertized  in the core network, because LPM is also applied for forwarding hint FIB lookup
9:55
Yes, actually we also consider the forwarding hint approach as an alternate.
9:56
I agree with your comments.  I thought that it requires rewriting,  Thank you so much

yoursunny  9:57 AM
Many people incorrectly believe that forwarding hint delegation name must equal network name such as /ndn/edu/arizona, but this isn't true. It could identify a network, a host, or even a program.
9:57
If you took my idea in the next publication, please give me a shout out in the acknowledgement section. Click on my photo to see my full name.

Sung Hyuk Byun  9:58 AM
Yes, of course!! I know you are Junxiao Shi already :slightly_smiling_face:

YONGYOON SHIN  5:53 PM
Sorry for the late reply.
@yoursunny Thank you for your opinion. I think it to be very helpful. I will check your comments and links. (edited) 
5:54
Dr. Sung Hyuk Byun is the CTO of the Chipmunk Project.
5:55
Really thank you very much for answering, Dr. @Sung Hyuk Byun (edited) 
```
