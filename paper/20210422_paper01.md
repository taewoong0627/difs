# Chipmunk: a Simple and Elastic NDN Repository

## ABSTRACT

In NDN, data should be kept as close as possible to edge so that it can be processed and analyzed closer to its source as in IP-based edge computing.
That will help reduce the traffic and meet the latency requirements of emerging applications more easily.
NDN is inherently equipped with the feature by supporting caching of data temporarily in network nodes.
In-network temporary caching serves the purpose for some popular traffic which are requested from lots of consumers at the same time.
But persistent storage is still required to store data so that it can be found and requested later regardless how popular it is.
As of today, some persistent storage solutions such as NDNFS and repo-ng have already been proposed and used for a few applications.
However, the existing solutions have shown lack of stability and scalability, especially in supporting large-sized data.
In this paper, we present a novel content-name based object store in NDN, Chipmunk, which is designed for distributed networks by separating actual file storage of data from its metadata having the information of the data including its location.
We show through the tests on the prototype implementation that Chipmunk provides more stability than and comparable performance to existing ones.
The paper also discusses how to improve the performance of Chipmunk as a future work.

## KEYWORDS

## 1. INTRODUCTION

Edge computing \cite{edgecomputing} is transforming the way data is being handled, processed, and delivered from millions of internet-connected devices around the world.
The explosive growth of the internet-connected devices along with emerging real-time applications, continues to drive edge computing technologies.
Edge computing does require bringing not only computation but also data storage closer to the devices where it’s being produced.
There are some available distributed data stores in IP networks such as IPFS (Interplanetary File System) \cite{ipfs} and HDFS (Hadoop Distributed File System) \cite{hadoop}.

NDN (Named Data Networking) is a future Internet architecture that includes support for security, delay tolerance, hop by hop routing, and mobility.
In particular, it is expected to solve traffic redundancy problems for existing IPv4-based Internet.
The CS (Content Store) of NDN can address the problem of traffic spikes due to the rapid increase in IoT devices.
NDN \cite{ndn} intrinsically has the capability to cache data which is being used frequently by many consumers, as close as to them. 
However, the in-network caching alone is not sufficient for general data storage which is required in network environment \cite{icn}.
There are a few NDN-based repositories such as repo-ng \cite{repong} and NDNFS \cite{ndnfs}, which are not designed for distributed networks.
They are also known to be unstable, especially for large-sized data. 
Let alone, NDN does not have any available distributed repository systems to the best of our knowledge, which is mainly due to a relatively short history of NDN.
In this paper, we propose a novel distributed repository, Chipmunk, extending repo-ng with many overall changes.
Chipmunk separates actual file storage of data from its metadata having the information of the data including its location; data is stored in distributed Chipmunk nodes in the vicinity of producers and its metadata having the information for the data is synchronized to other Chipmunk nodes. 
When a remote consumer, which is located in the other area from where the data it wants to download is produced, wants to download the data, it will find its actual repository by looking at the metadata first.

Chipmunk makes the following contributions:

- It provides an NDN based object storage and enable a filesystem as a service with horizontal scalability by choosing a distributed architecture instead of a centralized one.
- Applications leveraging Chipmunk can opt to use various different consistency policies such as eventual consistency, weak consistency, or no consistency to meet their requirements.
- It provides easy access for users (producers, consumers) who utilize chipmunk.
- It provides scalable persistent data storage based on NDN.
- It provides a data security method in a very simple way to reduce the overhead of users.

The rest of this paper is organized as follows.
Section 2 provides related work.
In Section 3, we describe the overall architecture and detailed procedures of Chipmunk.
In Section 4, we analyze the performance of Chipmunk. 
Section 5 covers concludes and future work to improve the Chipmunk.

## 2. RELATED WORK

### 2.1 repo-ng

Next generation NDN repository or repo-ng is an NDN-based network repository that supports insertion and retrieval of files using Content Names, built upon a relational database system.
It provides tools such as ndnputfile and ndngetfile in order to perform data insertion and retrieval.
The followings are function features of repo-ng:

- Data management: data and their names are stored and retrieved as BLOB type records.
- Provisioning of Tools: data insertion and retrieval are peformed using ndnputfile and ndngetfile.

repo-ng utilizes SQLite3 as underlying file storage.
repo-ng has demonstrated its potential as an NDN-based repository.
repo-ng has been adopted into several NDN applications as their repositories.

### 2.2 NDNFS

NDNFS (NDN-friendly file system) is a file system based on FUSE (Filesystem in userspace) \cite{fuse}.
It mounts a local directory to the NDN network using FUSE abstraction.
NDNFS features are:

- POSIX-style systems calls: it preserves the UNIX file operation interfaces when managing files (i.e., open, close, read, write, unlink and etc.).
- Remote-access: it offers remote access to its local directories by executing an NDNFS server.
- Metadata protocol: it queries requested data using embedded SQL.

NDNFS operates data in conformity with the FUSE interfaces.
FUSE executes a file system by bridging the user space to the kernel space through VFS (Virtual File System).
That is, the storage is mounted on the system like a NAS (Network Attached Storage) and you can copy or delete files using system commands.

### ndn-python-repo (추가 필요?)

An NDN Repository implementation using python-ndn \cite{pythonndn}.
ndn-python-repo features are:

-

## 3. DESIGN OF CHIPMUNK

저장소 서비스를 제공하는 사업자 또는 저장소가 필요한 프라이빗 클라우드에서 쉽고 간단하게 사용할 수 있어야 한다.
특히, 저장소를 사용할 사용자인 producer와 consumer가 간단하게 사용할 수 있도록 제공되어야 한다.
리전은 리전에서 발생한 데이터의 저장을 담당하는 저장소가 face 등록을 통하여 연결되어 있고 각 리전의 저장소들은 클러스터로 구성된다.

![](https://github.com/uni2u/difs/blob/documents/img/acm21-difs_region.png)

각 리전의 사용자들은 common name 을 사용하여 리전에 설정된 NDN 라우터에 등록된 Chipmunk 저장소로 접근한다.
Chipmunk 저장 관리자는 저장 노드 클러스터를 구성하며 node name 으로 서로 face 를 등록하여 연결한다.
Chipmunk 클러스터는 HDFS (Hadoop Distributed File System) 및 GFS (Google File System) 등의 디자인과 유사한 구조를 가진다.
클러스터를 관리하는 manager 노드를 통하여 클러스터를 구성하고 관리하며 클러스터 각 노드의 hash range 를 설정한다.
이와같은 디자인은 cassandra ring 디자인을 참조하였다.

![](https://github.com/uni2u/difs/blob/documents/img/acm21-ring00.png)

producer 의 데이터 저장 요청 Interest 를 수신한 노드는 NDN 데이터 패킷을 저장한다.
이때 producer 는 Chipmunk 저장소에 저장할 데이터의 service name 을 지정하며 저장 요청을 수신한 노드는 service name 을 해시하여 그 결과값을 각 노드마다 설정된 hash range 와 비교하고 manifest 파일을 제공할 노드를 선택한다.
manifest 파일에는 service name 에 대한 데이터 패킷을 실제 저장하고 있는 노드의 node name 이 포함된다.
consumer 는 리전에 상관없이 common name 및 service name 을 사용하여 데이터를 요청한다.
데이터 요청을 수신한 Chipmunk 노드는 service name 을 해시하고 그 결과를 각 노드마다 설정된 hash range 와 비교하고 manifest 제공 노드를 통해 manifest 파일로 응답한다.
이것은 같은 service name 을 해시한 결과는 같다는 hash 의 특성을 이용한 것으로 service name 에 대하여 일정한 manfiest 파일을 제공 노드를 선택할 수 있다.
consumer 는 manifest 파일을 확인하고 실제 데이터 패킷을 저장하고 있는 저장 노드에게 데이터 요청 Interest 를 전송하는 것으로 실제 데이터를 제공 받는다.

![](https://github.com/uni2u/difs/blob/documents/img/acm21-insertget.png)

As shown in Figure \ref{fig:insertion}, the data segments are stored in the following order:

- The producer requests to insert a data segment.
  - ndn://{common name}/insert
    - name: service name
- A Chipmunk node, which receives an insert request, pulls the data through the exchange of Interest and Data messages, and store it in the node.
- A metadata is created and the decision about selecting a node to store the metadata is made by a hash calculation on \textbf{\textit{service name}}.
- The request to store the metadata is delivered to the selected metadata store node.

As shown in Figure \ref{fig:retrieval}, the retrieval procedure of a data is performed as follows, regardless of the location of consumers:

- The consumer requests to get a data with \textbf{\textit{service name}}.
  - ndn://{common name}/get/{service name}
- The node that gets the request from the consumer, which is usually the closest to the consumer, find the metadata for the data.
- The metadata is returned to the consumer. The metadata has the information where the data is stored.
- The consumer sends a request to a node that actually stores the data.
  - ndn://{service name}/{segment number}
    - forwarding hint: node prefix
- It receives data from nodes that store actual data.

### 3.1 Common Name

Chipmunk 저장 클러스터의 사용자인 producer 및 consumer는 common name을 사용하여 간단하게 저장 서비스를 이용한다.
저장소를 구성하는 각 노드들은 node name 과 common name 을 가진다.
이러한 Interest 구조는 repo-ng 와 ndn-python-repo 에서도 확인할 수 있다.

- ndn://{CommonName}/{command}/{RepoCommandParameter}/{Signed Interest additional components}
  - command: key command to insert, get, delete data into repository.
  - RepoCommandParameter
    - name: either a Data packet name, or a name prefix of Data packets
    - StartBlockId (Optional): inclusive start segment number.
    - EndBlockId (Optional): inclusive end segment number.
    - ForwardingHint (Optional): forwarding hint for Data fetching.
    - RegisterPrefix(Optional): if repo doesn’t register the root prefix, client can tell repo to register this prefix.
    - CheckPrefix: a prefix of status check topic name.
    - ProcessId: a random byte string to identify this insertion process.

저장소 서비스 제공자는 사용자와의 계약을 통한 인증을 별도로 진행한다.
인증이 완료된 사용자는 CommonName 을 사용하여 쉽고 간단하게 Chipmunk 시스템으로 접근을 허용한다.
Chipmunk 저장소는 보다 쉽게 사용할 수 있도록 repo-ng 에서 제공하는 user tool 을 지원한다.

- ndnputfile {CommonName} {ServiceName} {DataPacketPrefix}
- ndngetfile {CommonName} {ServiceName}
- ndndelfile {CommonName} {ServiceName}

### 3.2 Storage Cluster

저장 서비스 제공자는 각 리전의 대표 Chipmunk 노드를 클러스터로 구성한다.
Chipmunk 노드는 데이터 패킷과 데이터의 메타데이터인 manifest 파일을 저장한다.
Chipmunk 관리자는 최초 노드를 설정하면서 노드의 config 를 통하여 manager 노드를 설정한다.

```
{
  "type": "manager"
}
```

클러스터는 manager 노드를 중심으로 구성되며 각 노드는 hash range 를 가진다.
최초 manager 노드는 모든 hash range 가 설정되어 데이터 및 manifest 파일을 저장하고 사용자에게 제공한다.
저장소 관리자는 리전이 추가되는 경우 또는 데이터가 증가하는 경우 Chipmunk 클러스터에 노드를 추가하기 위해 저장 노드를 manager 에게 알린다.

- ndn://{managerNodeName}/init
  - fromLeft: node name
  - fronRight (optional): node name

![](https://github.com/uni2u/difs/blob/documents/img/acm21-cluster.png)

command parameter 의 from 정보는 hash range 를 split 하는 위치를 의미한다.
즉, from 정보에 설정된 node name 이 가지고 있는 hash range 를 split 하는 것으로 전체 노드의 hash range 가 재 설정되는 문제를 피한다.
manager 노드는 각 노드의 hash range 를 keyspace 파일로 관리한다.
keyspace 의 version 으로 name 을 명명하고 keyspace 파일을 Chipmunk 클러스터 전체 노드로 배치한다.

- version announce Interest
  - ndn://{nodeName}/keyspace/ver/{keyspaceName}

Chipmunk 클러스터 각 노드는 manager 노드로부터 keyspace 파일 업데이트 알람을 받고 version (name) 을 비교한다.
비교 결과가 다른 경우 각 노드는 manager 노드로 keyspace 요청 Interest 를 전송하고 keyspace 파일을 제공 받는다.

- keyspace fetch Interest
  - ndn://{managerNodeName}/keyspace/fetch/{keyspaceName}

![](https://github.com/uni2u/difs/blob/documents/img/acm21-keyspace.png)

Chipmunk 관리자는 keyspace 파일을 생성하는 것으로 저장 서비스를 사용하는 리전이 추가되거나 특정 keyspace 범위의 파일이 증가하는 경우 저장 노드를 추가할 수 있다.
각 노드의 hash range 는 producer 가 저장을 요청하는 데이터의 manifest 파일을 네트워크에 균형적으로 분산하기 위하여 설정된다.
하지만 hash polarization 또는 hash imbalance 문제가 발생할 경우 특정 노드의 hash range 를 split 하지 위하여 저장 노드를 추가할 수 있다.
manager 노드가 생성 및 배포하는 keyspace 파일은 각 노드의 name 과 hash range 정보가 포함되어 있다.

```
{
  "keyspaces": [
    {
      "name": "node name",
      "start": "start hash num",
      "end": "end hash num"
    },
    {
      "name": "node name",
      "start": "start hash num",
      "end": "end hash num"
    },
    ...
  ]
}
```

이러한 방식을 통하여 Chipmunk 모든 노드는 같은 keyspace 파일을 가진다.
저장소 구성 변화는 Chipmunk 관리자에 의해서만 제어되며 빈번한 구성 변화는 일어나지 않을 것으로 예상하기 때문에 위와 같은 방식으로 충분하다.
추후, ChronoSync 또는 PSync 등의 NDN Dataset Synchronization 을 위한 특별한 기능을 사용할 수 있을 것으로 예상한다.

### 3.3 Manifest

producer 가 저장을 요청하면서 정의한 service name 으로 consumer 에게 제공하기 위해서는 producer 가 data prefix 를 announce 하여야 한다.
producer 가 저장 노드의 prefix 를 파악하고 등록하는 절차가 필요하며 결과적으로 producer 의 overhead 를 발생하게 된다.
또한, NDNS 및 NLSR 이 포함된 경우 producer 는 NDNS 와 NLSR 에 직접 등록하여야 한다.
현재 NDNS 은 tree 구조를 가지고 있으며 top tree 에 등록 되기까지 상당한 시간을 대기해야 한다.
현재 NLSR 은 sync protocol 을 통하여 모든 NLSR 의 동기화가 완료될 때까지 대기하여야 한다.
결과적으로 producer 의 overhead 가 증가한다.
Chipmunk 클러스터는 producer 의 네트워크 설정 부담을 줄이기 위하여 manifest 방식을 제공한다.
이것은 BitTorrent 의 swarm 에서 peer 의 주소를 제공하는 디자인을 참조하였다.
즉, service name 에 대한 메타데이터를 제공하는 Chipmunk 노드로 부터 데이터를 실제 저장하는 Chipmunk 노드의 name 을 확인할 수 있다.
메타데이터는 manifest 파일로 제공되며 manifest 에 포함된 데이터를 실제로 저장한 노드의 prefix 를 확인한 consumer 는 forwarding hint 를 적용하여 데이터 요청 interest 를 전송한다.

```
{
  "info": {
    "name": "service name",
    "hash": "service name hash result",
    "startBlockId": "start segment num",
    "endBlockId": "end segment num"
  },
  "storages": [
    {
      "storage_name": "real data stored node prefix",
      "segment": {
        "start_num": "start segment num",
        "end_num": "end segment num"
      }
    },
    ...
  ]
}
```

producer 의 데이터 저장 요청을 수신한 Chipmunk 노드는 data segments 를 저장하고 producer 가 정의한 service name 을 해시한다.
해시 결과는 데이터 저장 요청을 수신한 Chipmunk 노드가 가진 keyspace 파일을 통해 manifest 파일을 제공할 노드가 선택된다.
이러한 service name 을 해시하고 manifest 제공 노드를 선택하는 방식은 각 service name 에 대한 manifest 파일을 Chipmunk 클러스터 노드에 분산할 수 있다.

### 3.4 Signature

NDN 데이터 패킷은 데이터 보안을 위한 signature 필드가 있다.
signature 필드로 인하여 데이터를 안전하게 제공할 수 있는 장점이 있지만 이것으로 인하여 signature 를 생성하고 verification 하는 오버헤드도 발생한다.
특히, 데이터의 크기가 큰 경우 여러 조각의 data segments 로 나뉘어지기 때문에 각 패킷의 signature 를 생성하고 verification 을 한다면 매우 큰 오버헤드가 생성된다.
이러한 문제를 해결하는 방안으로 Hash Tree 방식의 FLIC 이 제안되어 있다.
하지만 FLIC 은 단순 signature 가 아닌 광범위한 문제를 다루고 있다.
우리는 보다 simple 한 방법으로 이 문제를 풀고자했고 hash chain 기법을 적용하는 것으로 해답을 얻었다.

![](https://github.com/uni2u/difs/blob/documents/img/acm21-hash01.png)

producer 가 최초 Chipmunk 클러스터에 데이터를 저장하기 위해 NDN 데이터 패킷을 hash chain 으로 구성한다.
NDN 데이터 패킷을 구성하기 위해 새그먼트로 만들어진 각 패킷을 뒤에서 부터 hash 를 계산한다.
만들어진 hash 를 이전 패킷의 데이터 필드에 추가하고 다시 hash 를 계산하여 이전 패킷에 포함하는 한다.
데이터의 첫번째 패킷에는 RSA signature 를 적용하는 것으로 hash chain 구성을 마친다.
저장 요청을 받은 Chipmunk 노드로 부터 데이터 요청 Interest 를 받으면 hash chain 으로 구성된 데이터 패킷을 첫번째 패킷부터 전송한다.

![](https://github.com/uni2u/difs/blob/documents/img/acm21-hash02.png)

Chipmunk 노드는 데이터 저장이 시작되면 varification 을 하는데 최초 첫 패킷의 RSA 를 제외한 나머지 데이터는 패킷에 포함된 next hash 를 비교하는 것으로 varification 과정을 마친다.
이러한 방식은 consumer 가 데이터를 요청하는 경우에도 그대로 활용되어 오버헤드를 크게 줄일 수 있다.

## 4. EXPERIMENTS

실험 내용

## 5. CONCLUSIONS AND FUTURE WORK



## ACKNOWLEDGMENTS

camera ready 시 작성하여야 함

## REFERENCES

@techreport{pythonndn,
    author = {},
    title = {},
    url = {https://python-ndn.readthedocs.io/en/latest/}
}
