# Chipmunk: a Simple and Elastic NDN Repository

## ABSTRACT

In NDN, data should be kept as close as possible to edge so that it can be processed and analyzed closer to its source as in IP-based edge computing.
That will help reduce the traffic and meet the latency requirements of emerging applications more easily.
NDN is inherently equipped with the feature by supporting caching of data temporarily in network nodes.
In-network temporary caching serves the purpose for some popular traffic which are requested from lots of consumers at the same time.
But persistent storage is still required to store data so that it can be found and requested later regardless how popular it is.
As of today, some persistent storage solutions such as NDNFS and repo-ng have already been proposed and used for a few applications.
However, the existing solutions have shown lack of stability and scalability, especially in supporting large-sized data.
In this paper, we present a novel content-name based object store in NDN, Chipmunk, which is designed for distributed networks by separating actual file storage of data from its metadata having the information of the data including its location.
We show through the tests on the prototype implementation that Chipmunk provides more stability than and comparable performance to existing ones.
The paper also discusses how to improve the performance of Chipmunk as a future work.

## KEYWORDS

## 1. INTRODUCTION

Edge computing \cite{edgecomputing} is transforming the way data is being handled, processed, and delivered from millions of internet-connected devices around the world.
The explosive growth of the internet-connected devices along with emerging real-time applications, continues to drive edge computing technologies.
Edge computing does require bringing not only computation but also data storage closer to the devices where it’s being produced.
There are some available distributed data stores in IP networks such as IPFS (Interplanetary File System) \cite{ipfs} and HDFS (Hadoop Distributed File System) \cite{hadoop}.

NDN (Named Data Networking) is a future Internet architecture that includes support for security, delay tolerance, hop by hop routing, and mobility.
In particular, it is expected to solve traffic redundancy problems for existing IPv4-based Internet.
The CS (Content Store) of NDN can address the problem of traffic spikes due to the rapid increase in IoT devices.
NDN \cite{ndn} intrinsically has the capability to cache data which is being used frequently by many consumers, as close as to them. 
However, the in-network caching alone is not sufficient for general data storage which is required in network environment \cite{icn}.
There are a few NDN-based repositories such as repo-ng \cite{repong} and NDNFS \cite{ndnfs}, which are not designed for distributed networks.
They are also known to be unstable, especially for large-sized data. 
Let alone, NDN does not have any available distributed repository systems to the best of our knowledge, which is mainly due to a relatively short history of NDN.
In this paper, we propose a novel simple elastic repository system, Chipmunk, extending repo-ng with many overall changes.
Chipmunk separates actual file storage of data from its metadata having the information of the data including its location; data is stored in distributed Chipmunk nodes in the vicinity of producers and its metadata having the information for the data is synchronized to other Chipmunk nodes. 
When a remote consumer, which is located in the other area from where the data it wants to download is produced, wants to download the data, it will find its actual repository by looking at the metadata first.

Chipmunk makes the following contributions:

- It provides an NDN based object storage and enable a filesystem as a service with horizontal scalability by choosing a distributed architecture instead of a centralized one.
- Applications leveraging Chipmunk can opt to use various different consistency policies such as eventual consistency, weak consistency, or no consistency to meet their requirements.
- It provides easy access for users (producers, consumers) who utilize chipmunk.
- It provides scalable persistent data storage based on NDN.
- It provides a data security method in a very simple way to reduce the overhead of users.

The rest of this paper is organized as follows.
Section 2 provides related work.
In Section 3, we describe the overall architecture and detailed procedures of Chipmunk.
In Section 4, we analyze the performance of Chipmunk. 
Section 5 covers concludes and future work to improve the Chipmunk.

## 2. RELATED WORK

### 2.1 repo-ng

Next generation NDN repository or repo-ng is an NDN-based network repository that supports insertion and retrieval of files using Content Names, built upon a relational database system.
It provides tools such as ndnputfile and ndngetfile in order to perform data insertion and retrieval.
The followings are function features of repo-ng:

- Data management: data and their names are stored and retrieved as BLOB type records.
- Provisioning of Tools: data insertion and retrieval are peformed using ndnputfile and ndngetfile.

repo-ng utilizes SQLite3 as underlying file storage.
repo-ng has demonstrated its potential as an NDN-based repository.
repo-ng has been adopted into several NDN applications as their repositories.

### 2.2 NDNFS

NDNFS (NDN-friendly file system) is a file system based on FUSE (Filesystem in userspace) \cite{fuse}.
It mounts a local directory to the NDN network using FUSE abstraction.
NDNFS features are:

- POSIX-style systems calls: it preserves the UNIX file operation interfaces when managing files (i.e., open, close, read, write, unlink and etc.).
- Remote-access: it offers remote access to its local directories by executing an NDNFS server.
- Metadata protocol: it queries requested data using embedded SQL.

NDNFS operates data in conformity with the FUSE interfaces.
FUSE executes a file system by bridging the user space to the kernel space through VFS (Virtual File System).
That is, the storage is mounted on the system like a NAS (Network Attached Storage) and you can copy or delete files using system commands.

### ndn-python-repo (추가 필요?)

An NDN Repository implementation using python-ndn \cite{pythonndn}.
ndn-python-repo features are:

-

## 3. DESIGN OF CHIPMUNK

In Chipmunk, we focus on tackling the following characteristics to satisfy the emerging demands for network storages and maximizes the benefits of adopting NDN.

- Productivity: the speed of data generation should be guaranteed when processing and storing data.
- Volume: new repositories should be able to store large files from tens of terabytes to tens of petabytes or more.
- Heterogeneity: data types should be classified according to the degrees of their structuredness of data. Data in the Relational database is considered structured data. JSON and XML formatted data are semi-structured data. Regular files such as video files and documents are considered unstructured data.
- Context-awareness: as the meaning changes in context, data also changes according to its usage.
- Visualization: it is about analyzing gathered data to extract information and presenting it in a way that users can understand more easily through visual aids.

The most well-known storage systems that satisfy the above characteristics are object storage systems \cite{objectstorage}.
In an Object storage, a file are broken up into small chunks and distributed over many computer storages.
Object storages offer independent storage and use unique identifiers to map objects and metadata to manage them.
It is especially popular as a cloud solution since it is convenient to meter the usage and offer a plan that pays for the metered usage.
We believe adopting the characteristics of object storage into an NDN repository is very desirable.

또한 저장소 서비스를 제공하는 사업자 또는 저장소가 필요한 프라이빗 클라우드에서 쉽고 간단하게 사용할 수 있어야 한다.
특히, 저장소를 사용할 사용자인 producer와 consumer가 간단하게 사용할 수 있도록 제공되어야 한다.
리전에서 발생한 데이터의 저장을 담당하는 저장소가 face 등록을 통하여 연결되어 있고 각 리전의 저장소들은 클러스터로 구성된다.

![](https://github.com/uni2u/difs/blob/documents/img/acm21-difs_region.png)

각 리전 사용자들은 common name 을 사용하여 리전의 NDN 라우터에 등록된 Chipmunk 저장소로 접근한다.
Chipmunk 저장 관리자는 저장 노드 클러스터를 구성하며 node name 으로 서로 face 를 등록하여 연결한다.
Chipmunk 클러스터는 HDFS (Hadoop Distributed File System) 및 GFS (Google File System) 등의 디자인과 유사한 구조를 가진다.
클러스터를 관리하는 manager 노드를 통하여 클러스터를 구성하고 관리하며 클러스터 각 노드의 hash range 를 설정한다.
이와같은 디자인은 cassandra ring 디자인을 참조하였다.

![](https://github.com/uni2u/difs/blob/documents/img/acm21-ring00.png)

producer 의 데이터 저장 요청 Interest 를 수신한 노드는 NDN 데이터 패킷을 저장한다.
이때 producer 는 Chipmunk 저장소에 저장할 데이터의 service name 을 지정하며 저장 요청을 수신한 노드는 service name 을 해시하여 그 결과값을 각 노드마다 설정된 hash range 와 비교하고 manifest 파일을 제공할 노드를 선택한다.
manifest 파일에는 service name 에 대한 데이터 패킷을 실제 저장하고 있는 노드의 node name 이 포함된다.
consumer 는 리전에 상관없이 common name 및 service name 을 사용하여 데이터를 요청한다.
데이터 요청을 수신한 Chipmunk 노드는 service name 을 해시하고 그 결과를 각 노드마다 설정된 hash range 와 비교하고 manifest 제공 노드를 통해 manifest 파일로 응답한다.
이것은 같은 service name 을 해시한 결과는 같다는 hash 의 특성을 이용한 것으로 service name 에 대하여 일정한 manfiest 파일을 제공 노드를 선택할 수 있다.
consumer 는 manifest 파일을 확인하고 실제 데이터 패킷을 저장하고 있는 저장 노드에게 데이터 요청 Interest 를 전송하는 것으로 실제 데이터를 제공 받는다.

![](https://github.com/uni2u/difs/blob/documents/img/acm21-insertget.png)

As shown in Figure \ref{fig:insertion}, the data segments are stored in the following order:

- The producer requests to insert a data segment.
  - ndn://{common name}/insert
    - name: service name
- A Chipmunk node, which receives an insert request, pulls the data through the exchange of Interest and Data messages, and store it in the node.
- A metadata is created and the decision about selecting a node to store the metadata is made by a hash calculation on \textbf{\textit{service name}}.
- The request to store the metadata is delivered to the selected metadata store node.

As shown in Figure \ref{fig:retrieval}, the retrieval procedure of a data is performed as follows, regardless of the location of consumers:

- The consumer requests to get a data with \textbf{\textit{service name}}.
  - ndn://{common name}/get/{service name}
- The node that gets the request from the consumer, which is usually the closest to the consumer, find the metadata for the data.
- The metadata is returned to the consumer. The metadata has the information where the data is stored.
- The consumer sends a request to a node that actually stores the data.
  - ndn://{service name}/{segment number}
    - forwarding hint: node prefix
- It receives data from nodes that store actual data.

### 3.1 Common Name

In the NDN, all objects (data, nodes, etc.) use the only name to represent themselves.
For example, the 'test.mp4' file in the '/data/music directory' of a node named 'c01' in a network named 'a/b' is named '/a/b/c01/data/music/test.mp4'.
This means that access is possible only when the network address, node name, and data name of the data provider are fully known.
This naming structure is inefficient in the data storage model.
Especially, it is very difficult to fully verify the storage location of each data in order to store a large number of data generated in the IoT environment. \par
Chipmunk is distributed repository that efficiently delivers data write/read through common repository naming.
In other words, it provides a consistent name.
It can be remotely connected to the repository through a common-name anywhere in the network.

Chipmunk 저장 클러스터의 사용자인 producer 및 consumer는 common name을 사용하여 간단하게 저장 서비스를 이용한다.
저장소를 구성하는 각 노드들은 node name 과 common name 을 가진다.
이러한 Interest 구조는 repo-ng 와 ndn-python-repo 에서도 확인할 수 있다.

The producer inserts the data into the repository using the common name and 'insert' commands in the repository.
The service-name of the data to be provided should not overlap in the Chipmunks network.

- ndn://{common name}/insert
  - RepoCommandParameter
    - name: either a Data packet name, or a name prefix of Data packets (same as service-name).
    - StartBlockId (Optional): inclusive start segment number.
    - EndBlockId (Optional): inclusive end segment number.
    - ForwardingHint (Optional): forwarding hint for Data fetching.
    - RegisterPrefix(Optional): if repo doesn’t register the root prefix, client can tell repo to register this prefix.
    - CheckPrefix: a prefix of status check topic name.
    - ProcessId: a random byte string to identify this insertion process.

The consumer receives data for the requested service-name using the common name and 'get' commands. \par

- ndn://{common name}/get/{service name}
  - data: manifest file
- ndn://{service name}/{segment number}
  - ForwardingHint: real data stored node prefix

All nodes that make up a Chipmunk cluster have common-names, and each node has its own node name.

Chipmunk also supports user tools provided by repo-ng for increased user convenience:

- ndnputfile {common-name} {service-name} {file-name}
- ndngetfile {common-name} {service-name}

User-tools plans to provide it as a library so that it can be easily used by users' applications.

### 3.2 Storage Cluster

저장 서비스 제공자는 각 리전의 대표 Chipmunk 노드를 클러스터로 구성한다.
Chipmunk 노드는 데이터 패킷과 데이터의 메타데이터인 manifest 파일을 저장한다.
Chipmunk 관리자는 최초 노드를 설정하면서 노드의 config 를 통하여 manager 노드를 설정한다.

```
{
  "type": "manager"
}
```

클러스터는 manager 노드를 중심으로 구성되며 각 노드는 hash range 를 가진다.
단일 manager 노드는 모든 hash range 가 설정되어 데이터 및 manifest 파일을 저장하고 사용자에게 제공한다.
저장소 관리자는 리전이 추가되거나 데이터가 증가하는 경우, Chipmunk 클러스터에 노드를 추가하기 위해 추가 저장 노드의 정보를 manager 노드에게 알린다.

- ndn://{managerNodeName}/init
  - fromLeft: target node name
  - fronRight (optional): target node name

![](https://github.com/uni2u/difs/blob/documents/img/acm21-cluster.png)

command parameter 의 from 정보는 hash range 를 split 하는 위치를 의미한다.
즉, from 정보에 정의된 node name 의 hash range 만 split 하는 것으로 전체 노드의 hash range 가 새로 설정되는 문제를 피한다.
manager 노드는 각 노드의 hash range 를 keyspace 파일로 관리한다.
keyspace 파일의 이름은 version 번호와 같고 keyspace 파일을 Chipmunk 클러스터 전체 노드로 배치한다.
keyspace 파일의 버전 정보를 알리는 Interest 는 다음과 같다:

- version announce Interest
  - ndn://{node-name}/keyspace/ver/{keyspace-name}

Chipmunk 클러스터 각 노드는 manager 노드로부터 keyspace 파일 업데이트 알람을 받고 version (name) 을 비교한다.
비교 결과가 다른 경우 각 노드는 manager 노드로 keyspace 요청 Interest 를 전송하고 keyspace 파일을 제공 받는다.
manager 노드로 보내는 keyspace 요청 Interest 형식은 다음과 같다:

- keyspace fetch Interest
  - ndn://{managerNodeName}/keyspace/fetch/{keyspaceName}

![](https://github.com/uni2u/difs/blob/documents/img/acm21-keyspace.png)

Chipmunk 관리자는 keyspace 파일을 생성하는 것으로 저장 서비스를 사용하는 리전이 추가되거나 특정 keyspace 범위의 파일이 증가하는 경우 저장 노드를 추가할 수 있다.
각 노드의 hash range 는 producer 가 저장을 요청하는 데이터의 manifest 파일을 네트워크에 균형적으로 분산하기 위하여 설정된다.
하지만 hash polarization 또는 hash imbalance 문제가 발생할 경우 특정 노드의 hash range 를 split 하지 위하여 저장 노드를 추가할 수 있다.
manager 노드가 생성 및 배포하는 keyspace 파일은 각 노드의 name 과 hash range 정보가 포함되어 있다.
keyspace 파일은 다음과 같다:

```
{
  "keyspaces": [
    {
      "name": "node name",
      "start": "start hash num",
      "end": "end hash num"
    },
    {
      "name": "node name",
      "start": "start hash num",
      "end": "end hash num"
    },
    ...
  ]
}
```

이러한 방식을 통하여 Chipmunk 모든 노드는 같은 keyspace 파일을 가진다.
저장소 구성 변화는 Chipmunk 관리자에 의해서만 제어되며 빈번한 구성 변화는 일어나지 않을 것으로 예상하기 때문에 위와 같은 방식으로 충분하다.
추후, ChronoSync 또는 PSync 등의 NDN Dataset Synchronization 을 위한 특별한 기능을 사용할 수 있을 것으로 예상한다.

### 3.3 Manifest Structure

producer 가 저장을 요청하면서 정의한 service name 으로 consumer 에게 제공하기 위해서는 producer 가 data prefix 를 announce 하여야 한다.
producer 가 저장 노드의 prefix 를 파악하고 등록하는 절차가 필요하며 결과적으로 producer 의 overhead 를 발생하게 된다.
또한, NDNS 및 NLSR 이 포함된 경우 producer 는 NDNS 와 NLSR 에 직접 등록하여야 한다.
현재 NDNS 은 tree 구조를 가지고 있으며 top tree 에 등록 되기까지 상당한 시간을 대기해야 한다.
현재 NLSR 은 sync protocol 을 통하여 모든 NLSR 의 동기화가 완료될 때까지 대기하여야 한다.
결과적으로 producer 의 overhead 가 증가한다.
Chipmunk 클러스터는 producer 의 네트워크 설정 부담을 줄이기 위하여 manifest 방식을 제공한다.
이것은 BitTorrent 의 swarm 에서 peer 의 주소를 제공하는 디자인을 참조하였다.
Chipmunk provide metadata of stored data using manifest files. 
The hash result of the data name requested by the data consumer refers to a specific node.
The selected Chipmunk node has a manifest that matches the service-name.
The manifest contains a description of the Data by the service-name.
It includes a hash value for a service-name, information about the Chipmunk node storing the data, and segment numbers.
Consumers can check the manifest to learn the details of the requested data.
As shown in the example in the Figure \ref{fig:manifest}, the data requester who has checked the details about the data through the manifest can receive the actual data by sending Interests to the Chipmunk node that stores the data.
Apply the prefix of the Chipmunk node that stores the actual data in the forwarding hint.
If there are many Chipmunk nodes storing the same data, a node list is provided through the storage field of the manifest, and by using this, Interests are sent to the Chipmunk node of the desired bandwidth and location and receive the data from the node.

메타데이터는 manifest 파일로 제공되며 manifest 에 포함된 데이터를 실제로 저장한 노드의 prefix 를 확인한 consumer 는 forwarding hint 를 적용하여 데이터 요청 interest 를 전송한다.

```
{
  "info": {
    "name": "service name",
    "hash": "service name hash result",
    "startBlockId": "start segment num",
    "endBlockId": "end segment num"
  },
  "storages": [
    {
      "storage_name": "real data stored node prefix",
      "segment": {
        "start_num": "start segment num",
        "end_num": "end segment num"
      }
    },
    ...
  ]
}
```

producer 의 데이터 저장 요청을 수신한 Chipmunk 노드는 data segments 를 저장하고 producer 가 정의한 service name 을 해시한다.
해시 결과는 데이터 저장 요청을 수신한 Chipmunk 노드가 가진 keyspace 파일을 통해 manifest 파일을 제공할 노드가 선택된다.
이러한 service name 을 해시하고 manifest 제공 노드를 선택하는 방식은 각 service name 에 대한 manifest 파일을 Chipmunk 클러스터 노드에 분산할 수 있다.

### 3.4 Data Signature

NDN adopts built-in security for all packets by utilizing a signature.
This undoubtedly enhances the security aspects, but at the same time, causes performance degradation.
As a default, a signature based on a public-key cryptosystem such as RSA is applied to each packet.
The default segment size is 8 kilobytes, so every segment needs to compute a signature which is a huge computing overhead.
To solve this issue, an embedded manifest \cite{embeddedmanifest} has been proposed.
However, in the case of a large file, N embedded manifests can be created, and as many signatures as N embedded manifests must be signed by a public-key encryption, which also shows potential insufficiency in dealing with large files. In order to provide a practical solution, a method that reduces the number of public-key signatures to a constant is desirable.
Chipmunk adopts the following method to solve the signature-overhead issues for dealing with large files.
이러한 문제를 해결하는 방안으로 Hash Tree 방식의 FLIC 이 제안되어 있다.
하지만 FLIC 은 단순 signature 가 아닌 광범위한 문제를 다루고 있다.
우리는 보다 simple 한 방법으로 이 문제를 풀고자했고 hash chain 기법을 적용하는 것으로 해답을 얻었다.
Chipmunk는 대용량 파일을 처리하기 위한 서명 오버헤드 문제를 해결하기 위해 다음과 같은 방법을 사용한다.

![](https://github.com/uni2u/difs/blob/documents/img/acm21-hash01.png)

producer 가 최초 Chipmunk 클러스터에 데이터를 저장하기 위해 NDN 데이터 패킷을 hash chain 으로 구성한다.
NDN 데이터 패킷을 구성하기 위해 새그먼트로 만들어진 각 패킷을 뒤에서 부터 hash 를 계산한다.
만들어진 hash 를 이전 패킷의 데이터 필드에 추가하고 다시 hash 를 계산하여 이전 패킷에 포함하는 한다.
데이터의 첫번째 패킷에는 RSA signature 를 적용하는 것으로 hash chain 구성을 마친다.
저장 요청을 받은 Chipmunk 노드로 부터 데이터 요청 Interest 를 받으면 hash chain 으로 구성된 데이터 패킷을 첫번째 패킷부터 전송한다.

![](https://github.com/uni2u/difs/blob/documents/img/acm21-hash02.png)

Chipmunk 노드는 데이터 저장이 시작되면 varification 을 하는데 최초 첫 패킷의 RSA 를 제외한 나머지 데이터는 패킷에 포함된 next hash 를 비교하는 것으로 varification 과정을 마친다.
이러한 방식은 consumer 가 데이터를 요청하는 경우에도 그대로 활용되어 오버헤드를 크게 줄일 수 있다.

#### 3.4.1 Hash Chain

Zibin and Shaoan et al. \cite{hashchain} explained a hash chain based signing solution that greatly decreases the public-key signing overhead in block-chain. It basically generates a hash chain that consists of hash values that connect each data, starting from the first Data packets to the last Data packet of a file as in Figure \ref{fig:hash-chain}.
For example, a producer generates a chain of 10 hashes for a file that is composed of 10 Data packets.
The Data packets are called {m\textsubscript{1}, m\textsubscript{2}, …, m\textsubscript{n}} and the hash values are called {k\textsubscript{1}, k\textsubscript{2}, …, k\textsubscript{n}}.
This method provides two ways of forming a hash chain a Backward Chain and a Forward Chain.
A backward chain is used when delayed authentication is tolerable and bandwidth is the most valuable.
Forward Chain fits well for applications that require real-time authentication.
However, both methods assume Data packets to arrive in a reasonable order.

The design of chaining approaches limits the use of the computationally expensive public key signing by opting to use more hashing operation.
Data packets are chained using a cryptographic hashing (i.e., SHA-256, or Blake2) and the chain is signed with a public-key signature.
The biggest strength of this approach is its simplicity to understand and to implement.

## 4. EXPERIMENTS

실험 내용

## 5. CONCLUSIONS AND FUTURE WORK



## ACKNOWLEDGMENTS

camera ready 시 작성하여야 함

## REFERENCES

@techreport{pythonndn,
    author = {},
    title = {},
    url = {https://python-ndn.readthedocs.io/en/latest/}
}
